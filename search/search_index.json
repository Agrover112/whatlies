{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WhatLies \u00b6 A library that tries help you to understand. \"What lies in word embeddings?\" Brief Introduction \u00b6 If you prefer a video tutorial before reading the getting started guide watch this; Produced \u00b6 This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa. What it Does \u00b6 This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out. Installation \u00b6 You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[docs] pip install whatlies[dev] pip install whatlies[test] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[ivis] pip install whatlies[opentsne] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it won't install all the language models you might want to visualise. Similar Projects \u00b6 There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here. Julia Bazi\u0144ska & Piotr Migdal Web App \u00b6 The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends. Tensorflow Projector \u00b6 From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it. Parallax \u00b6 From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental. Local Development \u00b6 If you want to develop locally you can start by running this command after cloning. make develop","title":"Home"},{"location":"#whatlies","text":"A library that tries help you to understand. \"What lies in word embeddings?\"","title":"WhatLies"},{"location":"#brief-introduction","text":"If you prefer a video tutorial before reading the getting started guide watch this;","title":"Brief Introduction"},{"location":"#produced","text":"This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa.","title":"Produced"},{"location":"#what-it-does","text":"This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out.","title":"What it Does"},{"location":"#installation","text":"You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[docs] pip install whatlies[dev] pip install whatlies[test] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[ivis] pip install whatlies[opentsne] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it won't install all the language models you might want to visualise.","title":"Installation"},{"location":"#similar-projects","text":"There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here.","title":"Similar Projects"},{"location":"#julia-bazinska-piotr-migdal-web-app","text":"The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends.","title":"Julia Bazi\u0144ska &amp; Piotr Migdal Web App"},{"location":"#tensorflow-projector","text":"From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it.","title":"Tensorflow Projector"},{"location":"#parallax","text":"From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental.","title":"Parallax"},{"location":"#local-development","text":"If you want to develop locally you can start by running this command after cloning. make develop","title":"Local Development"},{"location":"faq/","text":"F.A.Q. \u00b6 Plotting \u00b6 How do I save an interactive chart? \u00b6 The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here . How do I save an interactive chart for publication? \u00b6 You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk; Languages \u00b6 How do I access nearest tokens from a language model? \u00b6 This depends on the language model, please check the docs, but most language models will have a score_similar method attached. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return yield; [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens. How do I access nearest tokens from a language model using an embedding? \u00b6 You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn. How do I retreive an embedding set from language model using similar tokens? \u00b6 You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20)","title":"FAQ"},{"location":"faq/#faq","text":"","title":"F.A.Q."},{"location":"faq/#plotting","text":"","title":"Plotting"},{"location":"faq/#how-do-i-save-an-interactive-chart","text":"The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here .","title":"How do I save an interactive chart?"},{"location":"faq/#how-do-i-save-an-interactive-chart-for-publication","text":"You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk;","title":"How do I save an interactive chart for publication?"},{"location":"faq/#languages","text":"","title":"Languages"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model","text":"This depends on the language model, please check the docs, but most language models will have a score_similar method attached. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return yield; [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens.","title":"How do I access nearest tokens from a language model?"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model-using-an-embedding","text":"You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn.","title":"How do I access nearest tokens from a language model using an embedding?"},{"location":"faq/#how-do-i-retreive-an-embedding-set-from-language-model-using-similar-tokens","text":"You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20)","title":"How do I retreive an embedding set from language model using similar tokens?"},{"location":"roadmap/","text":"There's a few things that would be nice to have. Feel free to start a discussion on these topics in github. Multiple Plot Metrics At the moment we only project onto axes to get x/y coordinates. It might make sense to show the cosine distance to these axes instead. And if we're allowing cosine distance ... we might allow for flexible distance metrics in general. Table Summaries We've got a focus on charts now, but one imagines that calculating tables with summary statistics is also relevant. Languages It would be nice to have other language backends, given that we do not download all backends. We want this package to be light and users should download manually. language backends for huggingface models Testing it would be nice to have a good way of testing the charts it would be nice to be able to test multiple models without having to download gigabytes into github actions","title":"Roadmap"},{"location":"api/embedding/","text":"whatlies.embedding.Embedding \u00b6 This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar norm : (property, readonly) \u00b6 Gives the norm of the vector of the embedding __add__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __add__ ( self , other ) -> \"Embedding\" : \"\"\" Add two embeddings together. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo + bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } + { other . name } )\" copied . vector = self . vector + other . vector return copied Add two embeddings together. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo + bar __gt__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def __gt__ ( self , other ): \"\"\" Measures the size of one embedding to another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo > bar ``` \"\"\" return ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) Measures the size of one embedding to another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo > bar __or__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def __or__ ( self , other ): \"\"\" Makes one embedding orthogonal to the other one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } | { other . name } )\" copied . vector = self . vector - ( self >> other ) . vector return copied Makes one embedding orthogonal to the other one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __rshift__ ( self , other ): \"\"\" Maps an embedding unto another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo >> bar ``` \"\"\" copied = deepcopy ( self ) new_vec = ( ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) * other . vector ) copied . name = f \"( { self . name } >> { other . name } )\" copied . vector = new_vec return copied Maps an embedding unto another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo >> bar __sub__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __sub__ ( self , other ): \"\"\" Subtract two embeddings. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo - bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } - { other . name } )\" copied . vector = self . vector - other . vector return copied Subtract two embeddings. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo - bar distance ( self , other , metric = 'cosine' ) \u00b6 Show source code in whatlies/embedding.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" ) plot ( self , kind = 'scatter' , x_axis = None , y_axis = None , color = None , show_ops = False , annot = False ) \u00b6 Show source code in whatlies/embedding.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def plot ( self , kind : str = \"scatter\" , x_axis : Union [ str , \"Embedding\" ] = None , y_axis : Union [ str , \"Embedding\" ] = None , color : str = None , show_ops : bool = False , annot : bool = False , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if len ( self . vector ) == 2 : handle_2d_plot ( self , kind = kind , color = color , show_operations = show_ops , xlabel = x_axis , ylabel = y_axis , annot = annot , ) return self x_val = self > x_axis y_val = self > y_axis intermediate = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( intermediate , kind = kind , color = color , xlabel = x_axis . name , ylabel = y_axis . name , show_operations = show_ops , annot = annot , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis Union[str, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2 None y_axis Union[str, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated False Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"Embedding"},{"location":"api/embedding/#whatliesembeddingembedding","text":"This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar","title":"whatlies.embedding.Embedding"},{"location":"api/embedding/#whatlies.embedding.Embedding.norm","text":"Gives the norm of the vector of the embedding","title":"norm"},{"location":"api/embedding/#whatlies.embedding.Embedding.distance","text":"Show source code in whatlies/embedding.py 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" )","title":"distance()"},{"location":"api/embedding/#whatlies.embedding.Embedding.plot","text":"Show source code in whatlies/embedding.py 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 def plot ( self , kind : str = \"scatter\" , x_axis : Union [ str , \"Embedding\" ] = None , y_axis : Union [ str , \"Embedding\" ] = None , color : str = None , show_ops : bool = False , annot : bool = False , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if len ( self . vector ) == 2 : handle_2d_plot ( self , kind = kind , color = color , show_operations = show_ops , xlabel = x_axis , ylabel = y_axis , annot = annot , ) return self x_val = self > x_axis y_val = self > y_axis intermediate = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( intermediate , kind = kind , color = color , xlabel = x_axis . name , ylabel = y_axis . name , show_operations = show_ops , annot = annot , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis Union[str, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2 None y_axis Union[str, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated False Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"plot()"},{"location":"api/embeddingset/","text":"whatlies.embeddingset.EmbeddingSet \u00b6 This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of embeddings or dictionary with name: embedding.md pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar ) __add__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def __add__ ( self , other ): \"\"\" Adds an embedding to each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb + buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb + other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } + { other . name } )\" ) Adds an embedding to each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb + buz ) . plot ( kind = \"arrow\" ) __contains__ ( self , item ) \u00b6 Show source code in whatlies/embeddingset.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def __contains__ ( self , item ): \"\"\" Checks if an item is in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) \"foo\" in emb # True \"dinosaur\" in emb # False ``` \"\"\" return item in self . embeddings . keys () Checks if an item is in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) \"foo\" in emb # True \"dinosaur\" in emb # False __getitem__ ( self , thing ) \u00b6 Show source code in whatlies/embeddingset.py 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def __getitem__ ( self , thing ): \"\"\" Retreive a single embedding from the embeddingset. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz) emb[\"buz\"] ``` \"\"\" if isinstance ( thing , str ): return self . embeddings [ thing ] new_embeddings = { t : self [ t ] for t in thing } names = \",\" . join ( thing ) return EmbeddingSet ( new_embeddings , name = f \" { self . name } .subset( { names } )\" ) Retreive a single embedding from the embeddingset. Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) emb [ \"buz\" ] __iter__ ( self ) \u00b6 Show source code in whatlies/embeddingset.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __iter__ ( self ): \"\"\" Iterate over all the embeddings in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) [e for e in emb] ``` \"\"\" return self . embeddings . values () . __iter__ () Iterate over all the embeddings in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) [ e for e in emb ] __or__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def __or__ ( self , other ): \"\"\" Makes every element in the embeddingset othogonal to the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb | buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb | other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } | { other . name } )\" ) Makes every element in the embeddingset othogonal to the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb | buz ) . plot ( kind = \"arrow\" ) __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __rshift__ ( self , other ): \"\"\" Maps every embedding in the embedding set unto the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb >> buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb >> other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } >> { other . name } )\" ) Maps every embedding in the embedding set unto the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb >> buz ) . plot ( kind = \"arrow\" ) __sub__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __sub__ ( self , other ): \"\"\" Subtracts an embedding from each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb - buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb - other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } - { other . name } )\" ) Subtracts an embedding from each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb - buz ) . plot ( kind = \"arrow\" ) add_property ( self , name , func ) \u00b6 Show source code in whatlies/embeddingset.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' ) average ( self , name = None ) \u00b6 Show source code in whatlies/embeddingset.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5] embset_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings. filter ( self , func ) \u00b6 Show source code in whatlies/embeddingset.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name ) merge ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = em1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = em1 . merge ( emb2 ) movement_df ( self , other , metric = 'euclidean' ) \u00b6 Show source code in whatlies/embeddingset.py 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang1 = SpacyLanguage(\"en_core_web_sm\") lang2 = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb1 = lang1[names] emb2 = lang2[names] emb1.movement_df(emb2) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric )} ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang1 = SpacyLanguage ( \"en_core_web_sm\" ) lang2 = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb1 = lang1 [ names ] emb2 = lang2 [ names ] emb1 . movement_df ( emb2 ) plot ( self , kind = 'scatter' , x_axis = None , y_axis = None , color = None , show_ops = False , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def plot ( self , kind : str = \"scatter\" , x_axis : str = None , y_axis : str = None , color : str = None , show_ops : str = False , ** kwargs , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` \"\"\" for k , token in self . embeddings . items (): token . plot ( kind = kind , x_axis = x_axis , y_axis = y_axis , color = color , show_ops = show_ops , ** kwargs , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis str the x-axis to be used, must be given when dim > 2 None y_axis str the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops str setting to also show the applied operations, only works for text False plot_correlation ( self , metric = None ) \u00b6 Show source code in whatlies/embeddingset.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 def plot_correlation ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlation() ``` ![](https://rasahq.github.io/whatlies/images/corrplot.png) \"\"\" df = self . to_dataframe () . T corr_df = ( pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () ) fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) plt . show () Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlation () plot_interactive ( self , x_axis , y_axis , annot = True , show_axis_point = False , color = None ) \u00b6 Show source code in whatlies/embeddingset.py 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 def plot_interactive ( self , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], annot : bool = True , show_axis_point : bool = False , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] plot_df = pd . DataFrame ( { \"x_axis\" : self . compare_against ( x_axis ), \"y_axis\" : self . compare_against ( y_axis ), \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ([ x_axis . name , y_axis . name ])] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = f \" { x_axis . name } vs. { y_axis . name } \" ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' ) plot_interactive_matrix ( self , * axes , annot = True , show_axis_point = False , width = 200 , height = 200 ) \u00b6 Show source code in whatlies/embeddingset.py 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def plot_interactive_matrix ( self , * axes , annot : bool = True , show_axis_point : bool = False , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot, these should be in the embeddingset annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix('pca_0', 'pca_1', 'pca_2') ``` \"\"\" plot_df = pd . DataFrame ({ ax : self . compare_against ( self [ ax ]) for ax in axes }) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ( axes )] result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes [:: - 1 ], column = axes ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes the axes that we wish to plot, these should be in the embeddingset () annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) plot_movement ( self , other , x_axis , y_axis , first_group_name = 'before' , second_group_name = 'after' , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_difference(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_difference ( emb_new , 'man' , 'woman' ) plot_pixels ( self ) \u00b6 Show source code in whatlies/embeddingset.py 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels () score_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if str ( emb ) not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { str ( emb ) } ` does not exist in this EmbeddingSet\" ) if isinstance ( emb , str ): emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples. to_dataframe ( self ) \u00b6 Show source code in whatlies/embeddingset.py 427 428 429 430 431 432 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe. to_matrix ( self ) \u00b6 Show source code in whatlies/embeddingset.py 421 422 423 424 425 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array. to_X ( self ) \u00b6 Show source code in whatlies/embeddingset.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def to_X ( self ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X () to_X_y ( self , y_label ) \u00b6 Show source code in whatlies/embeddingset.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' ) transform ( self , transformer ) \u00b6 Show source code in whatlies/embeddingset.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"EmbeddingSet"},{"location":"api/embeddingset/#whatliesembeddingsetembeddingset","text":"This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of embeddings or dictionary with name: embedding.md pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar )","title":"whatlies.embeddingset.EmbeddingSet"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.add_property","text":"Show source code in whatlies/embeddingset.py 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' )","title":"add_property()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.average","text":"Show source code in whatlies/embeddingset.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5]","title":"average()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.embset_similar","text":"Show source code in whatlies/embeddingset.py 374 375 376 377 378 379 380 381 382 383 384 385 386 387 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.filter","text":"Show source code in whatlies/embeddingset.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name )","title":"filter()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.merge","text":"Show source code in whatlies/embeddingset.py 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = em1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = em1 . merge ( emb2 )","title":"merge()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.movement_df","text":"Show source code in whatlies/embeddingset.py 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang1 = SpacyLanguage(\"en_core_web_sm\") lang2 = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb1 = lang1[names] emb2 = lang2[names] emb1.movement_df(emb2) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric )} ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang1 = SpacyLanguage ( \"en_core_web_sm\" ) lang2 = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb1 = lang1 [ names ] emb2 = lang2 [ names ] emb1 . movement_df ( emb2 )","title":"movement_df()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot","text":"Show source code in whatlies/embeddingset.py 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 def plot ( self , kind : str = \"scatter\" , x_axis : str = None , y_axis : str = None , color : str = None , show_ops : str = False , ** kwargs , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` \"\"\" for k , token in self . embeddings . items (): token . plot ( kind = kind , x_axis = x_axis , y_axis = y_axis , color = color , show_ops = show_ops , ** kwargs , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis str the x-axis to be used, must be given when dim > 2 None y_axis str the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops str setting to also show the applied operations, only works for text False","title":"plot()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_correlation","text":"Show source code in whatlies/embeddingset.py 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 def plot_correlation ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlation() ``` ![](https://rasahq.github.io/whatlies/images/corrplot.png) \"\"\" df = self . to_dataframe () . T corr_df = ( pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () ) fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) plt . show () Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlation ()","title":"plot_correlation()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive","text":"Show source code in whatlies/embeddingset.py 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 def plot_interactive ( self , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], annot : bool = True , show_axis_point : bool = False , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] plot_df = pd . DataFrame ( { \"x_axis\" : self . compare_against ( x_axis ), \"y_axis\" : self . compare_against ( y_axis ), \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ([ x_axis . name , y_axis . name ])] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = f \" { x_axis . name } vs. { y_axis . name } \" ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' )","title":"plot_interactive()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive_matrix","text":"Show source code in whatlies/embeddingset.py 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 def plot_interactive_matrix ( self , * axes , annot : bool = True , show_axis_point : bool = False , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot, these should be in the embeddingset annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix('pca_0', 'pca_1', 'pca_2') ``` \"\"\" plot_df = pd . DataFrame ({ ax : self . compare_against ( self [ ax ]) for ax in axes }) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ( axes )] result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes [:: - 1 ], column = axes ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes the axes that we wish to plot, these should be in the embeddingset () annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"plot_interactive_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_movement","text":"Show source code in whatlies/embeddingset.py 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_difference(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_difference ( emb_new , 'man' , 'woman' )","title":"plot_movement()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_pixels","text":"Show source code in whatlies/embeddingset.py 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_md\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels ()","title":"plot_pixels()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.score_similar","text":"Show source code in whatlies/embeddingset.py 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if str ( emb ) not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { str ( emb ) } ` does not exist in this EmbeddingSet\" ) if isinstance ( emb , str ): emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_dataframe","text":"Show source code in whatlies/embeddingset.py 427 428 429 430 431 432 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe.","title":"to_dataframe()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_matrix","text":"Show source code in whatlies/embeddingset.py 421 422 423 424 425 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array.","title":"to_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X","text":"Show source code in whatlies/embeddingset.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def to_X ( self ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X ()","title":"to_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X_y","text":"Show source code in whatlies/embeddingset.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' )","title":"to_X_y()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.transform","text":"Show source code in whatlies/embeddingset.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"transform()"},{"location":"api/language/bpemb_lang/","text":"whatlies.language.BytePairLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/Users/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]] __getitem__ ( self , item ) \u00b6 Show source code in language/bpemblang.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __getitem__ ( self , item ): \"\"\" Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Arguments: item: single string or list of strings **Usage** ```python > lang = BytePairLanguage(lang=\"en\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] ``` \"\"\" if isinstance ( item , str ): with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = RuntimeWarning ) return Embedding ( item , self . module . embed ( item ) . mean ( axis = 0 )) if isinstance ( item , list ): return EmbeddingSet ( * [ self [ i ] for i in item ]) raise ValueError ( f \"Item must be list of string got { item } .\" ) Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Parameters Name Type Description Default item single string or list of strings required Usage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/bpemblang.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/bpemblang.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"BytePair"},{"location":"api/language/bpemb_lang/#whatlieslanguagebytepairlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/Users/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]]","title":"whatlies.language.BytePairLanguage"},{"location":"api/language/bpemb_lang/#whatlies.language.bpemblang.BytePairLanguage.embset_similar","text":"Show source code in language/bpemblang.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/bpemb_lang/#whatlies.language.bpemblang.BytePairLanguage.score_similar","text":"Show source code in language/bpemblang.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/convert_lang/","text":"whatlies.language.ConveRTLanguage \u00b6 This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. Also note that this language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Instead, you can create an embedding set by using this model and then perform similarity queries from there. Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, which could be one of 'convert , 'convert-multi-context' or 'convert-ubuntu' . Each one of these correspond to a different model as described in ConveRT manual . 'convert' signature str the TFHub signature of the model, which could be one of 'default' , 'encode_context' , 'encode_response' or 'encode_sequence' . Note that 'encode_context' is not currently supported with 'convert-multi-context' or 'convert-ubuntu' models. 'default' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage ( model_id = 'convert-multi-context' , signature = 'encode_sequence' ) > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/convert_lang.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage() > lang['bank'] > lang = ConveRTLanguage() > lang[['bank of the river', 'money on the bank', 'bank']] ``` \"\"\" if isinstance ( query , str ): query_tensor = tf . convert_to_tensor ([ query ]) encoding = self . model ( query_tensor ) if self . signature == \"encode_sequence\" : vec = encoding [ \"sequence_encoding\" ] . numpy () . sum ( axis = 1 )[ 0 ] else : vec = encoding [ \"default\" ] . numpy ()[ 0 ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage () > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]]","title":"ConveRT"},{"location":"api/language/convert_lang/#whatlieslanguageconvertlanguage","text":"This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. Also note that this language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Instead, you can create an embedding set by using this model and then perform similarity queries from there. Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, which could be one of 'convert , 'convert-multi-context' or 'convert-ubuntu' . Each one of these correspond to a different model as described in ConveRT manual . 'convert' signature str the TFHub signature of the model, which could be one of 'default' , 'encode_context' , 'encode_response' or 'encode_sequence' . Note that 'encode_context' is not currently supported with 'convert-multi-context' or 'convert-ubuntu' models. 'default' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage ( model_id = 'convert-multi-context' , signature = 'encode_sequence' ) > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]]","title":"whatlies.language.ConveRTLanguage"},{"location":"api/language/countvector_lang/","text":"whatlies.language.CountVectorLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/countvector_lang.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a set of embeddings. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang[['pizza', 'pizzas', 'firehouse', 'firehydrant']] ``` \"\"\" orig_str = isinstance ( query , str ) if orig_str : query = [ query ] if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) if self . fitted_manual : X = self . cv . transform ( query ) X_vec = self . svd . transform ( X ) else : X = self . cv . fit_transform ( query ) X_vec = self . svd . fit_transform ( X ) if orig_str : return Embedding ( name = query [ 0 ], vector = X_vec [ 0 ]) return EmbeddingSet ( * [ Embedding ( name = n , vector = v ) for n , v in zip ( query , X_vec )] ) Retreive a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/countvector_lang.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. fit_manual ( self , query ) \u00b6 Show source code in language/countvector_lang.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]] score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/countvector_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"CountVector"},{"location":"api/language/countvector_lang/#whatlieslanguagecountvectorlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]]","title":"whatlies.language.CountVectorLanguage"},{"location":"api/language/countvector_lang/#whatlies.language.countvector_lang.CountVectorLanguage.embset_similar","text":"Show source code in language/countvector_lang.py 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/countvector_lang/#whatlies.language.countvector_lang.CountVectorLanguage.fit_manual","text":"Show source code in language/countvector_lang.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]]","title":"fit_manual()"},{"location":"api/language/countvector_lang/#whatlies.language.countvector_lang.CountVectorLanguage.score_similar","text":"Show source code in language/countvector_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/fasttext_lang/","text":"whatlies.language.FasttextLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/fasttext_lang.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = FasttextLanguage(\"cc.en.300.bin\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): self . _input_str_legal ( query ) vec = self . model . get_word_vector ( query ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , top_n = 20000 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"fasttext"},{"location":"api/language/fasttext_lang/#whatlieslanguagefasttextlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.FasttextLanguage"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.embset_proximity","text":"Show source code in language/fasttext_lang.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.embset_similar","text":"Show source code in language/fasttext_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.score_similar","text":"Show source code in language/fasttext_lang.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/gensim_lang/","text":"whatlies.language.GensimLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/gensim_lang.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import GensimLanguage > lang = GensimLanguage(\"wordvectors.kv\") > lang['computer'] > lang = GensimLanguage(\"wordvectors.kv\") > lang[['computer', 'human', 'dog']] ``` \"\"\" if isinstance ( query , str ): if \" \" in query : return Embedding ( query , np . sum ([ self [ q ] . vector for q in query . split ( \" \" )], axis = 0 ) ) try : vec = np . sum ([ self . kv [ q ] for q in query . split ( \" \" )], axis = 0 ) except KeyError : vec = np . zeros ( self . kv . vector_size ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/gensim_lang.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/gensim_lang.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"Gensim"},{"location":"api/language/gensim_lang/#whatlieslanguagegensimlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]]","title":"whatlies.language.GensimLanguage"},{"location":"api/language/gensim_lang/#whatlies.language.gensim_lang.GensimLanguage.embset_similar","text":"Show source code in language/gensim_lang.py 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/gensim_lang/#whatlies.language.gensim_lang.GensimLanguage.score_similar","text":"Show source code in language/gensim_lang.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/s2v_lang/","text":"whatlies.language.Sense2VecLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here . __getitem__ ( self , query ) \u00b6 Show source code in language/sense2vec_lang.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __getitem__ ( self , query ): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['duck|NOUN'] > lang[['duck|NOUN'], ['duck|VERB']] ``` \"\"\" if isinstance ( query , str ): vec = self . s2v [ query ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'duck|NOUN' ] > lang [[ 'duck|NOUN' ], [ 'duck|VERB' ]] embset_similar ( self , query , n = 10 ) \u00b6 Show source code in language/sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , query , n = 10 ) \u00b6 Show source code in language/sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"Sense2Vec"},{"location":"api/language/s2v_lang/#whatlieslanguagesense2veclanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here .","title":"whatlies.language.Sense2VecLanguage"},{"location":"api/language/s2v_lang/#whatlies.language.sense2vec_lang.Sense2VecLanguage.embset_similar","text":"Show source code in language/sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/s2v_lang/#whatlies.language.sense2vec_lang.Sense2VecLanguage.score_similar","text":"Show source code in language/sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/spacy_lang/","text":"whatlies.language.SpacyLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] __getitem__ ( self , query ) \u00b6 Show source code in language/spacy_lang.py 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] > lang = SpacyLanguage(\"en_trf_robertabase_lg\") > lang['programming in [python]'] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] embset_proximity ( self , emb , max_proximity = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. from_fasttext ( language , output_dir , vectors_loc = None , force = False ) (classmethod) \u00b6 Show source code in language/spacy_lang.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are large and loading them can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_fasttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_fasttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are large and loading them can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_fasttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_fasttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" ) score_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"spaCy"},{"location":"api/language/spacy_lang/#whatlieslanguagespacylanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ]","title":"whatlies.language.SpacyLanguage"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.embset_proximity","text":"Show source code in language/spacy_lang.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.embset_similar","text":"Show source code in language/spacy_lang.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.from_fasttext","text":"Show source code in language/spacy_lang.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are large and loading them can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_fasttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_fasttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are large and loading them can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_fasttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_fasttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" )","title":"from_fasttext()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.score_similar","text":"Show source code in language/spacy_lang.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/tfhub/","text":"whatlies.language.TFHubLanguage \u00b6 This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. Also note that this language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Instead, you can create an embedding set by using this model and then perform similarity queries from there. Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/tfhub_lang.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import TFHubLanguage > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang['today is a gift'] > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang[['withdraw some money', 'take out cash', 'cash out funds']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"TFHub"},{"location":"api/language/tfhub/#whatlieslanguagetfhublanguage","text":"This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. Also note that this language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Instead, you can create an embedding set by using this model and then perform similarity queries from there. Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"whatlies.language.TFHubLanguage"},{"location":"api/language/transformers/","text":"whatlies.language.HFTransformersLanguage \u00b6 This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/hftransformers_lang.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: A single string or a list of strings Returns: An instance of [Embedding][whatlies.embedding.Embedding] (when `query` is a string) or [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] (when `query` is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). **Usage** ```python > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage('bert-base-cased') > lang['today is a nice day'] > lang = HFTransformersLanguage('gpt2') > lang[['day and night', 'it is as clear as day', 'today the sky is clear']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] A single string or a list of strings required Returns Type Description `` An instance of Embedding (when query is a string) or EmbeddingSet (when query is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). Usage > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"Huggingface"},{"location":"api/language/transformers/#whatlieslanguagehftransformerslanguage","text":"This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"whatlies.language.HFTransformersLanguage"},{"location":"api/transformers/addrandom/","text":"whatlies.transformers.AddRandom \u00b6 This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"Add Random"},{"location":"api/transformers/addrandom/#whatliestransformersaddrandom","text":"This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"whatlies.transformers.AddRandom"},{"location":"api/transformers/ivis/","text":"whatlies.transformers.Ivis \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of Ivis algorithm. We're using the implementation found here . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Ivis implementation {} Usage: from whatlies.language import GensimLanguage from whatlies.transformers import Ivis words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = GensimLanguage ( \"wordvectors.kv\" ) emb = lang [ words ] emb . transform ( Ivis ( 3 )) . plot_interactive_matrix ( 'ivis_0' , 'ivis_1' , 'ivis_2' )","title":"Ivis"},{"location":"api/transformers/ivis/#whatliestransformersivis","text":"This transformer scales all the vectors in an EmbeddingSet by means of Ivis algorithm. We're using the implementation found here . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Ivis implementation {} Usage: from whatlies.language import GensimLanguage from whatlies.transformers import Ivis words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = GensimLanguage ( \"wordvectors.kv\" ) emb = lang [ words ] emb . transform ( Ivis ( 3 )) . plot_interactive_matrix ( 'ivis_0' , 'ivis_1' , 'ivis_2' )","title":"whatlies.transformers.Ivis"},{"location":"api/transformers/noise/","text":"whatlies.transformers.Noise \u00b6 This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"Noise"},{"location":"api/transformers/noise/#whatliestransformersnoise","text":"This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"whatlies.transformers.Noise"},{"location":"api/transformers/opentsne/","text":"whatlies.transformers.OpenTsne \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation used open-tsne . Important OpenTSNE is a faster variant of TSNE but it only allows for <2 components. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the OpenTsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import OpenTsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( OpenTsne ( 2 )) . plot_interactive_matrix ( 'tsne_0' , 'tsne_1' )","title":"OpenTsne"},{"location":"api/transformers/opentsne/#whatliestransformersopentsne","text":"This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation used open-tsne . Important OpenTSNE is a faster variant of TSNE but it only allows for <2 components. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the OpenTsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import OpenTsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( OpenTsne ( 2 )) . plot_interactive_matrix ( 'tsne_0' , 'tsne_1' )","title":"whatlies.transformers.OpenTsne"},{"location":"api/transformers/pca/","text":"whatlies.transformers.Pca \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"Pca"},{"location":"api/transformers/pca/#whatliestransformerspca","text":"This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"whatlies.transformers.Pca"},{"location":"api/transformers/tsne/","text":"whatlies.transformers.Tsne \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 'tsne_0' , 'tsne_1' , 'tsne_2' )","title":"Tsne"},{"location":"api/transformers/tsne/#whatliestransformerstsne","text":"This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 'tsne_0' , 'tsne_1' , 'tsne_2' )","title":"whatlies.transformers.Tsne"},{"location":"api/transformers/umap/","text":"whatlies.transformers.Umap \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 'umap_0' , 'umap_1' , 'umap_2' )","title":"Umap"},{"location":"api/transformers/umap/#whatliestransformersumap","text":"This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 'umap_0' , 'umap_1' , 'umap_2' )","title":"whatlies.transformers.Umap"},{"location":"tutorial/embeddings/","text":"Imaginary Tokens \u00b6 Let's make a few word-embeddings. The basic object for this is a Token object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" ); Meaning \u00b6 Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' ); King - Man + Woman \u00b6 We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' ); King - Queen \u00b6 But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Man | (Queen - King) \u00b6 But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Embedding Mathmatics \u00b6 This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ] More Operations \u00b6 Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Mapping Unto Tokens \u00b6 In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Measuring the Mapping \u00b6 Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522 Interesting \u00b6 This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769 Plotting High Dimensions \u00b6 Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"What Are Embeddings"},{"location":"tutorial/embeddings/#imaginary-tokens","text":"Let's make a few word-embeddings. The basic object for this is a Token object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" );","title":"Imaginary Tokens"},{"location":"tutorial/embeddings/#meaning","text":"Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' );","title":"Meaning"},{"location":"tutorial/embeddings/#king-man-woman","text":"We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' );","title":"King - Man + Woman"},{"location":"tutorial/embeddings/#king-queen","text":"But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"King - Queen"},{"location":"tutorial/embeddings/#man-queen-king","text":"But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"Man | (Queen - King)"},{"location":"tutorial/embeddings/#embedding-mathmatics","text":"This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ]","title":"Embedding Mathmatics"},{"location":"tutorial/embeddings/#more-operations","text":"Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"More Operations"},{"location":"tutorial/embeddings/#mapping-unto-tokens","text":"In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"Mapping Unto Tokens"},{"location":"tutorial/embeddings/#measuring-the-mapping","text":"Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522","title":"Measuring the Mapping"},{"location":"tutorial/embeddings/#interesting","text":"This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769","title":"Interesting"},{"location":"tutorial/embeddings/#plotting-high-dimensions","text":"Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"Plotting High Dimensions"},{"location":"tutorial/embeddingsets/","text":"Sets of Embeddings \u00b6 The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an Direct Creation \u00b6 You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings. Via Languages \u00b6 But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] Plotting \u00b6 Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); Combining Charts \u00b6 Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web. More Transformation \u00b6 But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) plot_umap = umap_emb . plot_interactive ( 'umap_0' , 'umap_1' ) plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err }); Operators \u00b6 Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2() More Components \u00b6 Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) p2 = pca_emb . plot_interactive ( 'pca_2' , 'pca_1' ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err }); More Charts \u00b6 Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"Interactive Visualisation"},{"location":"tutorial/embeddingsets/#sets-of-embeddings","text":"The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an","title":"Sets of Embeddings"},{"location":"tutorial/embeddingsets/#direct-creation","text":"You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings.","title":"Direct Creation"},{"location":"tutorial/embeddingsets/#via-languages","text":"But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ]","title":"Via Languages"},{"location":"tutorial/embeddingsets/#plotting","text":"Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err });","title":"Plotting"},{"location":"tutorial/embeddingsets/#combining-charts","text":"Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web.","title":"Combining Charts"},{"location":"tutorial/embeddingsets/#more-transformation","text":"But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) plot_umap = umap_emb . plot_interactive ( 'umap_0' , 'umap_1' ) plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"More Transformation"},{"location":"tutorial/embeddingsets/#operators","text":"Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2()","title":"Operators"},{"location":"tutorial/embeddingsets/#more-components","text":"Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) p2 = pca_emb . plot_interactive ( 'pca_2' , 'pca_1' ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err });","title":"More Components"},{"location":"tutorial/embeddingsets/#more-charts","text":"Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"More Charts"},{"location":"tutorial/languages/","text":"In this tool we have support for different language backends and depending on the language backend you may get slightly different behavior. Multiple Tokens \u00b6 We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive ( 'pca_0' , 'pca_1' ) fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these. Bert Style \u00b6 But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please . Sense to Vec \u00b6 We also have support for the sense2vec model . To get it to work you first need to download and unzip the pretrained vectors found here but after that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN . Similarity \u00b6 Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" )) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err }); Scikit-Learn \u00b6 Some of the languages inside of this package can be used in scikit-learn pipelines. The spaCy and fasttext pipelines have compatible .fit() and .transform() methods implemented. That means that you could write code like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse\" , CountVectorizer ()) ]) Caveats \u00b6 There's a few caveats to be aware of though. Fasttext as well as spaCy cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the usecases should be limited to exploration, not production.","title":"Language Options"},{"location":"tutorial/languages/#multiple-tokens","text":"We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive ( 'pca_0' , 'pca_1' ) fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these.","title":"Multiple Tokens"},{"location":"tutorial/languages/#bert-style","text":"But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please .","title":"Bert Style"},{"location":"tutorial/languages/#sense-to-vec","text":"We also have support for the sense2vec model . To get it to work you first need to download and unzip the pretrained vectors found here but after that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN .","title":"Sense to Vec"},{"location":"tutorial/languages/#similarity","text":"Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" )) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err });","title":"Similarity"},{"location":"tutorial/languages/#scikit-learn","text":"Some of the languages inside of this package can be used in scikit-learn pipelines. The spaCy and fasttext pipelines have compatible .fit() and .transform() methods implemented. That means that you could write code like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse\" , CountVectorizer ()) ])","title":"Scikit-Learn"},{"location":"tutorial/languages/#caveats","text":"There's a few caveats to be aware of though. Fasttext as well as spaCy cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the usecases should be limited to exploration, not production.","title":"Caveats"},{"location":"tutorial/observations/","text":"Observations \u00b6 This document will demonstrate some observations of embeddings. The goal of this document is to two-fold. It hopes to explain how to use whatlies . It hopes to explain interesting elements of different embeddings. Spelling Errors \u00b6 Especially when you're designing a chatbot, spelling errors occur all the time. So how do different embeddings deal with this? Let's compare three different language backends here. from whatlies.language import FasttextLanguage , SpacyLanguage , CountVectorLanguage lang_spacy = SpacyLanguage ( \"en_core_web_md\" ) lang_fasttext = FasttextLanguage ( \"cc.en.300.bin\" ) lang_cv = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 3 ), analyzer = \"char\" ) lang_cv . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' , 'cat' , 'dog' , 'pikachu' , 'pokemon' ]) Besides fetching a fasttext and spaCy model you'll notice that we're also manually fitting the CountVectorLanguage . Take a note of this because this fact will be meaningful later. words = [ 'piza' , 'pizza' , 'pizzza' , 'italian' , 'sushi' , 'japan' , 'burger' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' , 'house' , 'tree' , 'elephant' , 'pikachu' , 'pokemon' ] def mk_plot ( lang ): return ( lang [ words ] . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) . properties ( height = 250 , width = 250 , title = lang . __class__ . __name__ )) ( mk_plot ( lang_spacy ) & mk_plot ( lang_fasttext ) & mk_plot ( lang_cv )) fetch('chart-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); You may notice a few things here. The CountVectorLanguage only looks at character similarities which it is why it feels that \"piza\", \"pizza\" and \"pizzza\" are all very similar. Notice how spaCy disagrees with this. You may also note that fasttext is \"in between\". That is because these embeddings also encode the subtokens. The fasttext embeddings seem to do a better job at catching certain forms of meaning. It understands that pikachu, pokemon and japan are related while also associating burger/sushi with pizza. The spaCy model also captures this but the clustering is less appearant. This can be due to the dimensionality reduction though. You'll notice that the two clearly misspelled words, fyrehouse and fyrehidrant, are mapped to the same point in the spaCy embedding. When you check the embeddings for both you'll confirm why. SpaCy maps a token to a vector of zeros is it is not available in the vocabulary. SpaCy may occasionally also map two different tokens to the same embedding in an attempt to save on disk space. Fasttext is able to recover more context because of the subtoken embeddings but also because the embeddings are way bigger . The fasttext embeddings unzipped can be 7GB on disk while spaCy model is on 115MB on disk. Notice how the CountVectorLanguage has a cluster with lots of things in it. It clusters together \"house\", \"elephant\", \"sushi\" and \"pokemon\". This isn't because of shared meaning. It because these words contain combinations of characters that weren't there in the training set that we gave to the fit_manual method in the beginning. Note that \"picachu\" is similar to \"pizza\" for a similar reason.","title":"Observations"},{"location":"tutorial/observations/#observations","text":"This document will demonstrate some observations of embeddings. The goal of this document is to two-fold. It hopes to explain how to use whatlies . It hopes to explain interesting elements of different embeddings.","title":"Observations"},{"location":"tutorial/observations/#spelling-errors","text":"Especially when you're designing a chatbot, spelling errors occur all the time. So how do different embeddings deal with this? Let's compare three different language backends here. from whatlies.language import FasttextLanguage , SpacyLanguage , CountVectorLanguage lang_spacy = SpacyLanguage ( \"en_core_web_md\" ) lang_fasttext = FasttextLanguage ( \"cc.en.300.bin\" ) lang_cv = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 3 ), analyzer = \"char\" ) lang_cv . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' , 'cat' , 'dog' , 'pikachu' , 'pokemon' ]) Besides fetching a fasttext and spaCy model you'll notice that we're also manually fitting the CountVectorLanguage . Take a note of this because this fact will be meaningful later. words = [ 'piza' , 'pizza' , 'pizzza' , 'italian' , 'sushi' , 'japan' , 'burger' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' , 'house' , 'tree' , 'elephant' , 'pikachu' , 'pokemon' ] def mk_plot ( lang ): return ( lang [ words ] . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) . properties ( height = 250 , width = 250 , title = lang . __class__ . __name__ )) ( mk_plot ( lang_spacy ) & mk_plot ( lang_fasttext ) & mk_plot ( lang_cv )) fetch('chart-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); You may notice a few things here. The CountVectorLanguage only looks at character similarities which it is why it feels that \"piza\", \"pizza\" and \"pizzza\" are all very similar. Notice how spaCy disagrees with this. You may also note that fasttext is \"in between\". That is because these embeddings also encode the subtokens. The fasttext embeddings seem to do a better job at catching certain forms of meaning. It understands that pikachu, pokemon and japan are related while also associating burger/sushi with pizza. The spaCy model also captures this but the clustering is less appearant. This can be due to the dimensionality reduction though. You'll notice that the two clearly misspelled words, fyrehouse and fyrehidrant, are mapped to the same point in the spaCy embedding. When you check the embeddings for both you'll confirm why. SpaCy maps a token to a vector of zeros is it is not available in the vocabulary. SpaCy may occasionally also map two different tokens to the same embedding in an attempt to save on disk space. Fasttext is able to recover more context because of the subtoken embeddings but also because the embeddings are way bigger . The fasttext embeddings unzipped can be 7GB on disk while spaCy model is on 115MB on disk. Notice how the CountVectorLanguage has a cluster with lots of things in it. It clusters together \"house\", \"elephant\", \"sushi\" and \"pokemon\". This isn't because of shared meaning. It because these words contain combinations of characters that weren't there in the training set that we gave to the fit_manual method in the beginning. Note that \"picachu\" is similar to \"pizza\" for a similar reason.","title":"Spelling Errors"},{"location":"tutorial/transformations/","text":"State and Colors \u00b6 A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( 'umap_0' , 'umap_1' , color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); Visualising Differences \u00b6 Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize. Movement \u00b6 If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Transformation Viz"},{"location":"tutorial/transformations/#state-and-colors","text":"A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( 'umap_0' , 'umap_1' , color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err });","title":"State and Colors"},{"location":"tutorial/transformations/#visualising-differences","text":"Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize.","title":"Visualising Differences"},{"location":"tutorial/transformations/#movement","text":"If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Movement"}]}