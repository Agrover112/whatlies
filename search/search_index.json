{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WhatLies \u00b6 A library that tries help you to understand. \"What lies in word embeddings?\" Brief Introduction \u00b6 If you prefer a video tutorial before reading the getting started guide watch this; Produced \u00b6 This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa. What it Does \u00b6 This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out. Installation \u00b6 You can install the package via pip; pip install whatlies This will install dependencies but it won't install all the language models you might want to visualise. Similar Projects \u00b6 There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here. Julia Bazi\u0144ska & Piotr Migdal Web App \u00b6 The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends. Tensorflow Projector \u00b6 From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it. Parallax \u00b6 From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental. Local Development \u00b6 If you want to develop locally you can start by running this command after cloning. make develop","title":"Home"},{"location":"#whatlies","text":"A library that tries help you to understand. \"What lies in word embeddings?\"","title":"WhatLies"},{"location":"#brief-introduction","text":"If you prefer a video tutorial before reading the getting started guide watch this;","title":"Brief Introduction"},{"location":"#produced","text":"This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa.","title":"Produced"},{"location":"#what-it-does","text":"This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out.","title":"What it Does"},{"location":"#installation","text":"You can install the package via pip; pip install whatlies This will install dependencies but it won't install all the language models you might want to visualise.","title":"Installation"},{"location":"#similar-projects","text":"There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here.","title":"Similar Projects"},{"location":"#julia-bazinska-piotr-migdal-web-app","text":"The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends.","title":"Julia Bazi\u0144ska &amp; Piotr Migdal Web App"},{"location":"#tensorflow-projector","text":"From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it.","title":"Tensorflow Projector"},{"location":"#parallax","text":"From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental.","title":"Parallax"},{"location":"#local-development","text":"If you want to develop locally you can start by running this command after cloning. make develop","title":"Local Development"},{"location":"roadmap/","text":"There's a few things that would be nice to have. Feel free to start a discussion on these topics in github. Multiple Plot Metrics At the moment we only project onto axes to get x/y coordinates. It might make sense to show the cosine distance to these axes instead. And if we're allowing cosine distance ... we might allow for flexible distance metrics in general. Table Summaries We've got a focus on charts now, but one imagines that calculating tables with summary statistics is also relevant. Difference Charts It might be nice to show where a point was before a transformation and then show where it is after . Especially in the realm of de-biasing this feels very interesting; show where the embeddig was before and then show where it is now. Languages It would be nice to have other language backends, given that we do not download all backends. We want this package to be light and users should download manually. language backends for huggingface models language backends for gensim models Testing it would be nice to have a good way of testing the charts it would be nice to be able to test multiple models without having to download gigabytes into github actions More plots There may be visualisations worth adding that we've not considered. Like the bar chart discussed in this github issue .","title":"Roadmap"},{"location":"api/embedding/","text":"whatlies.embedding.Embedding \u00b6 This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar foo - bar + bar __add__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 def __add__ ( self , other ) -> \"Embedding\" : \"\"\" Add two embeddings together. Usage: ``` from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo + bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } + { other . name } )\" copied . vector = self . vector + other . vector return copied Add two embeddings together. Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo + bar __gt__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __gt__ ( self , other ): \"\"\" Measures the size of one embedding to another one. Usage: ``` from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo > bar ``` \"\"\" return ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) Measures the size of one embedding to another one. Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo > bar __or__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __or__ ( self , other ): \"\"\" Makes one embedding orthogonal to the other one. Usage: ``` from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } | { other . name } )\" copied . vector = self . vector - ( self >> other ) . vector return copied Makes one embedding orthogonal to the other one. Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 def __rshift__ ( self , other ): \"\"\" Maps an embedding unto another one. Usage: ``` from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo >> bar ``` \"\"\" copied = deepcopy ( self ) new_vec = ( ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) * other . vector ) copied . name = f \"( { self . name } >> { other . name } )\" copied . vector = new_vec return copied Maps an embedding unto another one. Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo >> bar __sub__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __sub__ ( self , other ): \"\"\" Subtract two embeddings. Usage: ``` from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo - bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } - { other . name } )\" copied . vector = self . vector - other . vector return copied Subtract two embeddings. Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo - bar plot ( self , kind = 'scatter' , x_axis = None , y_axis = None , color = None , show_ops = False , annot = False ) \u00b6 Show source code in whatlies/embedding.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def plot ( self , kind : str = \"scatter\" , x_axis : Union [ str , 'Embedding' ] = None , y_axis : Union [ str , 'Embedding' ] = None , color : str = None , show_ops : bool = False , annot : bool = False , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if len ( self . vector ) == 2 : handle_2d_plot ( self , kind = kind , color = color , show_operations = show_ops , xlabel = x_axis , ylabel = y_axis , annot = annot , ) return self x_val = self > x_axis y_val = self > y_axis intermediate = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( intermediate , kind = kind , color = color , xlabel = x_axis . name , ylabel = y_axis . name , show_operations = show_ops , annot = annot , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis Union[str, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2 None y_axis Union[str, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated False Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"Embedding"},{"location":"api/embedding/#whatliesembeddingembedding","text":"This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar foo - bar + bar","title":"whatlies.embedding.Embedding"},{"location":"api/embedding/#whatlies.embedding.Embedding.plot","text":"Show source code in whatlies/embedding.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 def plot ( self , kind : str = \"scatter\" , x_axis : Union [ str , 'Embedding' ] = None , y_axis : Union [ str , 'Embedding' ] = None , color : str = None , show_ops : bool = False , annot : bool = False , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if len ( self . vector ) == 2 : handle_2d_plot ( self , kind = kind , color = color , show_operations = show_ops , xlabel = x_axis , ylabel = y_axis , annot = annot , ) return self x_val = self > x_axis y_val = self > y_axis intermediate = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( intermediate , kind = kind , color = color , xlabel = x_axis . name , ylabel = y_axis . name , show_operations = show_ops , annot = annot , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis Union[str, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2 None y_axis Union[str, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated False Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"plot()"},{"location":"api/embeddingset/","text":"whatlies.embeddingset.EmbeddingSet \u00b6 This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of embeddings or dictionary with name: embedding.md pairs name : custom name of embeddingset Usage: from whatlies.embedding.md import Embedding from whatlies.embeddingset import EmbeddingSet __add__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def __add__ ( self , other ): \"\"\" Adds an embedding to each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb + buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb + other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } + { other . name } )\" ) Adds an embedding to each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb + buz ) . plot ( kind = \"arrow\" ) __contains__ ( self , item ) \u00b6 Show source code in whatlies/embeddingset.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 def __contains__ ( self , item ): \"\"\" Checks if an item is in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) \"foo\" in emb # True \"dinosaur\" in emb # False ``` \"\"\" return item in self . embeddings . keys () Checks if an item is in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) \"foo\" in emb # True \"dinosaur\" in emb # False __getitem__ ( self , thing ) \u00b6 Show source code in whatlies/embeddingset.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def __getitem__ ( self , thing ): \"\"\" Retreive a single embedding from the embeddingset. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz) emb[\"buz\"] ``` \"\"\" if not isinstance ( thing , list ): return self . embeddings [ thing ] new_embeddings = { k : emb for k , emb in self . embeddings . items ()} names = \",\" . join ( thing ) return EmbeddingSet ( new_embeddings , name = f \" { self . name } .subset( { names } )\" ) Retreive a single embedding from the embeddingset. Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) emb [ \"buz\" ] __iter__ ( self ) \u00b6 Show source code in whatlies/embeddingset.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __iter__ ( self ): \"\"\" Iterate over all the embeddings in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) [e for e in emb] ``` \"\"\" return self . embeddings . values () . __iter__ () Iterate over all the embeddings in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) [ e for e in emb ] __or__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def __or__ ( self , other ): \"\"\" Makes every element in the embeddingset othogonal to the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb | buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb | other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } | { other . name } )\" ) Makes every element in the embeddingset othogonal to the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb | buz ) . plot ( kind = \"arrow\" ) __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def __rshift__ ( self , other ): \"\"\" Maps every embedding in the embedding set unto the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb >> buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb >> other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } >> { other . name } )\" ) Maps every embedding in the embedding set unto the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb >> buz ) . plot ( kind = \"arrow\" ) __sub__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def __sub__ ( self , other ): \"\"\" Subtracts an embedding from each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb - buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb - other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } - { other . name } )\" ) Subtracts an embedding from each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb - buz ) . plot ( kind = \"arrow\" ) add_property ( self , name , func ) \u00b6 Show source code in whatlies/embeddingset.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' ) average ( self , name = None ) \u00b6 Show source code in whatlies/embeddingset.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = np . array ([ v . vector for v in self . embeddings . values ()]) return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5] embset_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings. merge ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = em1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = em1 . merge ( emb2 ) plot ( self , kind = 'scatter' , x_axis = None , y_axis = None , color = None , show_ops = False , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def plot ( self , kind : str = \"scatter\" , x_axis : str = None , y_axis : str = None , color : str = None , show_ops : str = False , ** kwargs , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` \"\"\" for k , token in self . embeddings . items (): token . plot ( kind = kind , x_axis = x_axis , y_axis = y_axis , color = color , show_ops = show_ops , ** kwargs , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis str the x-axis to be used, must be given when dim > 2 None y_axis str the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops str setting to also show the applied operations, only works for text False plot_correlaton ( self , metric = None ) \u00b6 Show source code in whatlies/embeddingset.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def plot_correlaton ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Usage: ```python from whatlies.language import SpacyLanguage names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlaton() ``` ![](/images/corrplot.png) \"\"\" df = self . to_dataframe () . T corr_df = pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) plt . show () Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Usage: from whatlies.language import SpacyLanguage names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlaton () plot_interactive ( self , x_axis , y_axis , annot = True , show_axis_point = False , color = None ) \u00b6 Show source code in whatlies/embeddingset.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def plot_interactive ( self , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], annot : bool = True , show_axis_point : bool = False , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] plot_df = pd . DataFrame ({ \"x_axis\" : self . compare_against ( x_axis ), \"y_axis\" : self . compare_against ( y_axis ), \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], }) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else '' for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ([ x_axis . name , y_axis . name ])] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = f \" { x_axis . name } vs. { y_axis . name } \" ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' ) plot_interactive_matrix ( self , * axes , annot = True , show_axis_point = False , width = 200 , height = 200 ) \u00b6 Show source code in whatlies/embeddingset.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def plot_interactive_matrix ( self , * axes , annot : bool = True , show_axis_point : bool = False , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot, these should be in the embeddingset annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix('pca_0', 'pca_1', 'pca_2') ``` \"\"\" plot_df = pd . DataFrame ({ ax : self . compare_against ( self [ ax ]) for ax in axes }) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ( axes )] result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes [:: - 1 ], column = axes ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes the axes that we wish to plot, these should be in the embeddingset () annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) plot_movement ( self , other , x_axis , y_axis , first_group_name = 'before' , second_group_name = 'after' , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_difference(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( 'original' ) . drop ( columns = [ 'name' ])) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( 'original' ) . drop ( columns = [ 'name' ]) . loc [ lambda d : d . index . isin ( df1 . index )]) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ 'original' ]) . assign ( constant = 1 )) plots = [] for idx , grp_df in df_draw . groupby ( 'original' ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = 'x_axis:Q' , y = 'y_axis:Q' )) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" )) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" )) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_difference ( emb_new , 'man' , 'woman' ) score_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if str ( emb ) not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { str ( emb ) } ` does not exist in this EmbeddingSet\" ) if isinstance ( emb , str ): emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = np . array ([ w . vector for w in self . embeddings . values ()]) distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples. to_X ( self ) \u00b6 Show source code in whatlies/embeddingset.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def to_X ( self ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X () to_X_y ( self , y_label ) \u00b6 Show source code in whatlies/embeddingset.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = np . array ([ e . vector for e in self . embeddings . values ()]) y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' ) transform ( self , transformer ) \u00b6 Show source code in whatlies/embeddingset.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"EmbeddingSet"},{"location":"api/embeddingset/#whatliesembeddingsetembeddingset","text":"This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of embeddings or dictionary with name: embedding.md pairs name : custom name of embeddingset Usage: from whatlies.embedding.md import Embedding from whatlies.embeddingset import EmbeddingSet","title":"whatlies.embeddingset.EmbeddingSet"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.add_property","text":"Show source code in whatlies/embeddingset.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' )","title":"add_property()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.average","text":"Show source code in whatlies/embeddingset.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = np . array ([ v . vector for v in self . embeddings . values ()]) return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5]","title":"average()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.embset_similar","text":"Show source code in whatlies/embeddingset.py 349 350 351 352 353 354 355 356 357 358 359 360 361 362 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.merge","text":"Show source code in whatlies/embeddingset.py 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = em1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = em1 . merge ( emb2 )","title":"merge()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot","text":"Show source code in whatlies/embeddingset.py 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def plot ( self , kind : str = \"scatter\" , x_axis : str = None , y_axis : str = None , color : str = None , show_ops : str = False , ** kwargs , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` \"\"\" for k , token in self . embeddings . items (): token . plot ( kind = kind , x_axis = x_axis , y_axis = y_axis , color = color , show_ops = show_ops , ** kwargs , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'scatter' x_axis str the x-axis to be used, must be given when dim > 2 None y_axis str the y-axis to be used, must be given when dim > 2 None color str the color of the dots None show_ops str setting to also show the applied operations, only works for text False","title":"plot()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_correlaton","text":"Show source code in whatlies/embeddingset.py 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def plot_correlaton ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Usage: ```python from whatlies.language import SpacyLanguage names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlaton() ``` ![](/images/corrplot.png) \"\"\" df = self . to_dataframe () . T corr_df = pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) plt . show () Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Usage: from whatlies.language import SpacyLanguage names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlaton ()","title":"plot_correlaton()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive","text":"Show source code in whatlies/embeddingset.py 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 def plot_interactive ( self , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], annot : bool = True , show_axis_point : bool = False , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] plot_df = pd . DataFrame ({ \"x_axis\" : self . compare_against ( x_axis ), \"y_axis\" : self . compare_against ( y_axis ), \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], }) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else '' for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ([ x_axis . name , y_axis . name ])] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = f \" { x_axis . name } vs. { y_axis . name } \" ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_axis . name )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_axis . name )), text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' )","title":"plot_interactive()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive_matrix","text":"Show source code in whatlies/embeddingset.py 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 def plot_interactive_matrix ( self , * axes , annot : bool = True , show_axis_point : bool = False , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot, these should be in the embeddingset annot: drawn points should be annotated show_axis_point: ensure that the axis are drawn width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix('pca_0', 'pca_1', 'pca_2') ``` \"\"\" plot_df = pd . DataFrame ({ ax : self . compare_against ( self [ ax ]) for ax in axes }) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] if not show_axis_point : plot_df = plot_df . loc [ lambda d : ~ d [ \"name\" ] . isin ( axes )] result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes [:: - 1 ], column = axes ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes the axes that we wish to plot, these should be in the embeddingset () annot bool drawn points should be annotated True show_axis_point bool ensure that the axis are drawn False width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"plot_interactive_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_movement","text":"Show source code in whatlies/embeddingset.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_difference(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( 'original' ) . drop ( columns = [ 'name' ])) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( 'original' ) . drop ( columns = [ 'name' ]) . loc [ lambda d : d . index . isin ( df1 . index )]) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ 'original' ]) . assign ( constant = 1 )) plots = [] for idx , grp_df in df_draw . groupby ( 'original' ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = 'x_axis:Q' , y = 'y_axis:Q' )) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" )) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , show_axis_point = True , color = \"group\" )) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_difference ( emb_new , 'man' , 'woman' )","title":"plot_movement()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.score_similar","text":"Show source code in whatlies/embeddingset.py 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if str ( emb ) not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { str ( emb ) } ` does not exist in this EmbeddingSet\" ) if isinstance ( emb , str ): emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = np . array ([ w . vector for w in self . embeddings . values ()]) distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X","text":"Show source code in whatlies/embeddingset.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def to_X ( self ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X ()","title":"to_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X_y","text":"Show source code in whatlies/embeddingset.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = np . array ([ e . vector for e in self . embeddings . values ()]) y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' )","title":"to_X_y()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.transform","text":"Show source code in whatlies/embeddingset.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"transform()"},{"location":"api/language/fasttext_lang/","text":"whatlies.language.FasttextLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/fasttext_lang.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = FasttextLanguage(\"cc.en.300.bin\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): self . _input_str_legal ( query ) vec = self . ft . get_word_vector ( query ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , top_n = 20000 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ({ w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity }) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/fasttext_lang.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"fasttext"},{"location":"api/language/fasttext_lang/#whatlieslanguagefasttextlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.FasttextLanguage"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.embset_proximity","text":"Show source code in language/fasttext_lang.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ({ w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity }) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.embset_similar","text":"Show source code in language/fasttext_lang.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/fasttext_lang/#whatlies.language.fasttext_lang.FasttextLanguage.score_similar","text":"Show source code in language/fasttext_lang.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/s2v_lang/","text":"whatlies.language.Sense2VecLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here . __getitem__ ( self , query ) \u00b6 Show source code in language/sense2vec_lang.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __getitem__ ( self , query ): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['duck|NOUN'] > lang[['duck|NOUN'], ['duck|VERB']] ``` \"\"\" if isinstance ( query , str ): vec = self . s2v [ query ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'duck|NOUN' ] > lang [[ 'duck|NOUN' ], [ 'duck|VERB' ]] embset_similar ( self , query , n = 10 ) \u00b6 Show source code in language/sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , query , n = 10 ) \u00b6 Show source code in language/sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"Sense2Vec"},{"location":"api/language/s2v_lang/#whatlieslanguagesense2veclanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here .","title":"whatlies.language.Sense2VecLanguage"},{"location":"api/language/s2v_lang/#whatlies.language.sense2vec_lang.Sense2VecLanguage.embset_similar","text":"Show source code in language/sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/s2v_lang/#whatlies.language.sense2vec_lang.Sense2VecLanguage.score_similar","text":"Show source code in language/sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/spacy_lang/","text":"whatlies.language.SpacyLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default model Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] __getitem__ ( self , query ) \u00b6 Show source code in language/spacy_lang.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): self . _input_str_legal ( query ) start , end = _selected_idx_spacy ( query ) clean_string = query . replace ( \"[\" , \"\" ) . replace ( \"]\" , \"\" ) vec = self . nlp ( clean_string )[ start : end ] . vector return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ({ w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity }) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. from_fasttext ( language , output_dir , vectors_loc = None , force = False ) (classmethod) \u00b6 Show source code in language/spacy_lang.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are big that and loading this in can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_texttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_texttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are big that and loading this in can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_texttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_texttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" ) score_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/spacy_lang.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"spaCy"},{"location":"api/language/spacy_lang/#whatlieslanguagespacylanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default model Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ]","title":"whatlies.language.SpacyLanguage"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.embset_proximity","text":"Show source code in language/spacy_lang.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ({ w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity }) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.embset_similar","text":"Show source code in language/spacy_lang.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.from_fasttext","text":"Show source code in language/spacy_lang.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are big that and loading this in can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_texttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_texttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are big that and loading this in can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_texttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_texttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" )","title":"from_fasttext()"},{"location":"api/language/spacy_lang/#whatlies.language.spacy_lang.SpacyLanguage.score_similar","text":"Show source code in language/spacy_lang.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/transformers/addrandom/","text":"whatlies.transformers.AddRandom \u00b6 This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"Add Random"},{"location":"api/transformers/addrandom/#whatliestransformersaddrandom","text":"This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"whatlies.transformers.AddRandom"},{"location":"api/transformers/noise/","text":"whatlies.transformers.Noise \u00b6 This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"Noise"},{"location":"api/transformers/noise/#whatliestransformersnoise","text":"This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"whatlies.transformers.Noise"},{"location":"api/transformers/pca/","text":"whatlies.transformers.Pca \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"Pca"},{"location":"api/transformers/pca/#whatliestransformerspca","text":"This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' )","title":"whatlies.transformers.Pca"},{"location":"api/transformers/umap/","text":"whatlies.transformers.Umap \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 'umap_0' , 'umap_1' , 'umap_2' )","title":"Umap"},{"location":"api/transformers/umap/#whatliestransformersumap","text":"This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 'umap_0' , 'umap_1' , 'umap_2' )","title":"whatlies.transformers.Umap"},{"location":"tutorial/embeddings/","text":"Imaginary Tokens \u00b6 Let's make a few word-embeddings. The basic object for this is a Token object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" ); Meaning \u00b6 Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' ); King - Man + Woman \u00b6 We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' ); King - Queen \u00b6 But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Man | (Queen - King) \u00b6 But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Embedding Mathmatics \u00b6 This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ] More Operations \u00b6 Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Mapping Unto Tokens \u00b6 In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Measuring the Mapping \u00b6 Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522 Interesting \u00b6 This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769 Plotting High Dimensions \u00b6 Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier was to deal with collections of embeddings.","title":"What Are Embeddings"},{"location":"tutorial/embeddings/#imaginary-tokens","text":"Let's make a few word-embeddings. The basic object for this is a Token object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" );","title":"Imaginary Tokens"},{"location":"tutorial/embeddings/#meaning","text":"Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' );","title":"Meaning"},{"location":"tutorial/embeddings/#king-man-woman","text":"We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' );","title":"King - Man + Woman"},{"location":"tutorial/embeddings/#king-queen","text":"But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"King - Queen"},{"location":"tutorial/embeddings/#man-queen-king","text":"But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"Man | (Queen - King)"},{"location":"tutorial/embeddings/#embedding-mathmatics","text":"This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ]","title":"Embedding Mathmatics"},{"location":"tutorial/embeddings/#more-operations","text":"Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"More Operations"},{"location":"tutorial/embeddings/#mapping-unto-tokens","text":"In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"Mapping Unto Tokens"},{"location":"tutorial/embeddings/#measuring-the-mapping","text":"Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522","title":"Measuring the Mapping"},{"location":"tutorial/embeddings/#interesting","text":"This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769","title":"Interesting"},{"location":"tutorial/embeddings/#plotting-high-dimensions","text":"Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier was to deal with collections of embeddings.","title":"Plotting High Dimensions"},{"location":"tutorial/embeddingsets/","text":"Sets of Embeddings \u00b6 The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an Direct Creation \u00b6 You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings. Via Languages \u00b6 But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] Plotting \u00b6 Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); Combining Charts \u00b6 Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web. More Transformation \u00b6 But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) plot_umap = umap_emb . plot_interactive ( 'umap_0' , 'umap_1' ) plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err }); Operators \u00b6 Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2() More Components \u00b6 Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) p2 = pca_emb . plot_interactive ( 'pca_2' , 'pca_1' ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err }); More Charts \u00b6 Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"Interactive Visualisation"},{"location":"tutorial/embeddingsets/#sets-of-embeddings","text":"The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an","title":"Sets of Embeddings"},{"location":"tutorial/embeddingsets/#direct-creation","text":"You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings.","title":"Direct Creation"},{"location":"tutorial/embeddingsets/#via-languages","text":"But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ]","title":"Via Languages"},{"location":"tutorial/embeddingsets/#plotting","text":"Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err });","title":"Plotting"},{"location":"tutorial/embeddingsets/#combining-charts","text":"Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web.","title":"Combining Charts"},{"location":"tutorial/embeddingsets/#more-transformation","text":"But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) plot_umap = umap_emb . plot_interactive ( 'umap_0' , 'umap_1' ) plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"More Transformation"},{"location":"tutorial/embeddingsets/#operators","text":"Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2()","title":"Operators"},{"location":"tutorial/embeddingsets/#more-components","text":"Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive ( 'pca_0' , 'pca_1' ) p2 = pca_emb . plot_interactive ( 'pca_2' , 'pca_1' ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err });","title":"More Components"},{"location":"tutorial/embeddingsets/#more-charts","text":"Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 'pca_0' , 'pca_1' , 'pca_2' ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"More Charts"},{"location":"tutorial/languages/","text":"In this tool we have support for different language backends and depending on the language backend you may get slightly different behavior. Multiple Tokens \u00b6 We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive ( 'pca_0' , 'pca_1' ) fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these. Bert Style \u00b6 But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please . Sense to Vec \u00b6 We also have support for the sense2vec model . To get it to work you first need to download and unzip the pretrained vectors found here but after that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN . Similarity \u00b6 Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" )) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err });","title":"Language Options"},{"location":"tutorial/languages/#multiple-tokens","text":"We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive ( 'pca_0' , 'pca_1' ) fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these.","title":"Multiple Tokens"},{"location":"tutorial/languages/#bert-style","text":"But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please .","title":"Bert Style"},{"location":"tutorial/languages/#sense-to-vec","text":"We also have support for the sense2vec model . To get it to work you first need to download and unzip the pretrained vectors found here but after that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" ) p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN .","title":"Sense to Vec"},{"location":"tutorial/languages/#similarity","text":"Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ( \"pca_0\" , \"pca_1\" )) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err });","title":"Similarity"},{"location":"tutorial/transformations/","text":"State and Colors \u00b6 A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( 'umap_0' , 'umap_1' , color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); Visualising Differences \u00b6 Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize. Movement \u00b6 If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Transformation Viz"},{"location":"tutorial/transformations/#state-and-colors","text":"A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( 'umap_0' , 'umap_1' , color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err });","title":"State and Colors"},{"location":"tutorial/transformations/#visualising-differences","text":"Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize.","title":"Visualising Differences"},{"location":"tutorial/transformations/#movement","text":"If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Movement"}]}