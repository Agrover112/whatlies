{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two notebooks might have gotten your attention but usually we get the response; \n",
    "\n",
    "> But what about BERT-embeddings? \n",
    "\n",
    "Let's explain that a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import Token, TokenSet\n",
    "import spacy \n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "nlp = spacy.load(\"en_trf_robertabase_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Token Embeddings\n",
    "\n",
    "We can also have embeddings that represent more than one token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_context_embedding(string):\n",
    "    for idx, word in enumerate(string.split(\" \")):\n",
    "        if word[0] == \"[\":\n",
    "            new_string = string.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "            spacy_token = nlp(new_string)[idx].vector\n",
    "            return Token(string, spacy_token)\n",
    "    return Token(string, nlp(string).vector)\n",
    "\n",
    "contexts = (\"this snake is a python\",\n",
    "            \"i like to program in python\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i go to the supermarket\",\n",
    "            \"i like to code\", \n",
    "            \"i love animals\")\n",
    "\n",
    "t = TokenSet({k: grab_context_embedding(k) for k in contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = grab_context_embedding(\"python is for programming\")\n",
    "y_axis = grab_context_embedding(\"snakes are slimy creatures\")\n",
    "t.plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis).plot(kind=\"scatter\", x_axis=x_axis, y_axis=y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings of Tokens with Context\n",
    "\n",
    "We can go a step further too. If we have the sentence `this snake is a python` then an algorithm like Bert will not apply seperate word embeddings for each token. Rather, the entire document will first learn it's representation before assigning it to seperate tokens. If you are interested in a Bert representation of a word given the context that it is in ... you can get them with a special syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = (\"i put my money on the [bank]\",\n",
    "            \"the water flows on the river [bank]\",\n",
    "            \"i really like swimming in the [ocean]\",\n",
    "            \"i have plenty of [cash] on me\",)\n",
    "\n",
    "t = TokenSet({k: grab_context_embedding(k) for k in contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = grab_context_embedding(\"[money] is important to my cash flow\")\n",
    "y_axis = grab_context_embedding(\"[rivers] are great for swimming\")\n",
    "t.plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis).plot(kind=\"scatter\", x_axis=x_axis, y_axis=y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realisation \n",
    "\n",
    "This is the demo so-far.\n",
    "\n",
    "We need to change the `Token` part of the library. This should be called an embedding. That way we can keep everything in the same object! \n",
    "\n",
    "Still ... this is a nice preview of what is possible. \n",
    "\n",
    "Future Features; \n",
    "\n",
    "- if the plotting backend is plotly/c3 then we should be able to host interactive versions on the blog\n",
    "- we might want to have a `LazyEmbeddingSet` such that we can also compute distances between tokens lazily, it would also be nice to have an object where we can say \"use this backend\" so that we might compare plots for different trained embeddings\n",
    "- there are also other transformations that you might want to apply ... PCA for example ...\n",
    "\n",
    "That said; give all feedback to Vincent and he will **WorkWork[tm]**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
