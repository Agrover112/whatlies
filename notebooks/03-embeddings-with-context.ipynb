{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two notebooks might have gotten your attention but usually we get the response; \n",
    "\n",
    "> But what about BERT-embeddings? \n",
    "\n",
    "Let's explain how to get there, but first ... we should explain languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import Embedding, EmbeddingSet\n",
    "import spacy \n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Token Embeddings\n",
    "\n",
    "We can also have embeddings that represent more than one token. If we'd do this via spacy, we'd have a an average of all the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "contexts = (\"this snake is a python\",\n",
    "            \"i like to program in python\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i go to the supermarket\",\n",
    "            \"i like to code\", \n",
    "            \"i love animals\")\n",
    "\n",
    "t = EmbeddingSet({k: Embedding(k, nlp(k).vector) for k in contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_str, y_str = \"python is for programming\", \"snakes are slimy creatures\"\n",
    "x_axis = Embedding(x_str, nlp(x_str).vector)\n",
    "y_axis = Embedding(y_str, nlp(y_str).vector)\n",
    "t.plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis).plot(kind=\"scatter\", x_axis=x_axis, y_axis=y_axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings of Tokens with Context\n",
    "\n",
    "But maybe we'd like to have BERT-style models. These models work differently. Luckily ... spaCy also supports this these days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_trf_robertabase_lg\")\n",
    "\n",
    "contexts = (\"this snake is a python\",\n",
    "            \"i like to program in python\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i go to the supermarket\",\n",
    "            \"i like to code\", \n",
    "            \"i love animals\")\n",
    "\n",
    "t = EmbeddingSet({k: Embedding(k, nlp(k).vector) for k in contexts})\n",
    "\n",
    "x_str, y_str = \"python is for programming\", \"snakes are slimy creatures\"\n",
    "x_axis = Embedding(x_str, nlp(x_str).vector)\n",
    "y_axis = Embedding(y_str, nlp(y_str).vector)\n",
    "t.plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis).plot(kind=\"scatter\", x_axis=x_axis, y_axis=y_axis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go a step further too. If we have the sentence `this snake is a python` then an algorithm like Bert will not apply seperate word embeddings for each token. Rather, the entire document will first learn it's representation before assigning it to seperate tokens. If you are interested in a Bert representation of a word given the context that it is in ... you can get them with a special syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = (\"i put my money on the [bank]\",\n",
    "            \"the water flows on the river [bank]\",\n",
    "            \"i really like [to swim] in water\",\n",
    "            \"i want to be so rich that i am [drowning] in money\",\n",
    "            \"i have plenty of [cash] on me\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to make use of this syntax we need a new object; the `Language` object. This is a tool for `whatlies` to grab the appropriate word embeddings on your behalf. It will handle the context but can also be seen as a lazy `EmbeddingSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies.language import SpacyLanguage\n",
    "\n",
    "lang = SpacyLanguage(\"en_trf_robertabase_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = EmbeddingSet({k: lang[k] for k in contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = lang[\"money is important to my cash flow\"]\n",
    "y_axis = lang[\"a beach is next to the ocean\"]\n",
    "\n",
    "(t\n",
    " .plot(kind=\"text\", x_axis=x_axis, y_axis=y_axis)\n",
    " .plot(kind=\"scatter\", x_axis=x_axis, y_axis=y_axis))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realisation \n",
    "\n",
    "This is the demo so-far. Still ... this is a nice preview of what is possible. \n",
    "\n",
    "Future Features; \n",
    "\n",
    "- if the plotting backend is plotly/c3 then we should be able to host interactive versions on the blog\n",
    "- we might want to have a `LazyEmbeddingSet` such that we can also compute distances between tokens lazily, it would also be nice to have an object where we can say \"use this backend\" so that we might compare plots for different trained embeddings\n",
    "- allow the creation of a random vector too\n",
    "- there are also other transformations that you might want to apply ... PCA for example ...\n",
    "\n",
    "That said; give all feedback to Vincent and he will **WorkWork[tm]**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
