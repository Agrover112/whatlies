{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous two notebooks might have gotten your attention but usually we get the response; \n",
    "\n",
    "> But what about BERT-embeddings? \n",
    "\n",
    "Let's explain how to get there, but first ... we should explain languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import Embedding, EmbeddingSet\n",
    "import spacy \n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Token Embeddings\n",
    "\n",
    "We can also have embeddings that represent more than one token. If we'd do this via spacy, we'd have a an average of all the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-c4c16c947d5249f8a38ea3ec8b605bac\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-c4c16c947d5249f8a38ea3ec8b605bac\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": {\"type\": \"circle\", \"size\": 60}, \"encoding\": {\"tooltip\": [{\"type\": \"nominal\", \"field\": \"name\"}, {\"type\": \"nominal\", \"field\": \"original\"}], \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"pca_0\"}, \"field\": \"x_axis\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"pca_1\"}, \"field\": \"y_axis\"}}, \"selection\": {\"selector005\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"pca_0 vs. pca_1\"}, {\"mark\": {\"type\": \"text\", \"color\": \"black\", \"dx\": -15, \"dy\": 3}, \"encoding\": {\"text\": {\"type\": \"nominal\", \"field\": \"original\"}, \"x\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"pca_0\"}, \"field\": \"x_axis\"}, \"y\": {\"type\": \"quantitative\", \"axis\": {\"title\": \"pca_1\"}, \"field\": \"y_axis\"}}}], \"data\": {\"name\": \"data-200f9cc82a1daf6e7be3033441372c0c\"}, \"height\": 400, \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-200f9cc82a1daf6e7be3033441372c0c\": [{\"x_axis\": -3.250830888748169, \"y_axis\": 1.1827195882797241, \"name\": \"i am super duper happy\", \"original\": \"i am super duper happy\"}, {\"x_axis\": 5.148332118988037, \"y_axis\": -5.458179950714111, \"name\": \"happy happy joy joy\", \"original\": \"happy happy joy joy\"}, {\"x_axis\": -6.476804733276367, \"y_axis\": -5.5691938400268555, \"name\": \"programming is super fun!\", \"original\": \"programming is super fun!\"}, {\"x_axis\": -3.914728879928589, \"y_axis\": 7.071300983428955, \"name\": \"i am going crazy i hate it\", \"original\": \"i am going crazy i hate it\"}, {\"x_axis\": 8.494030952453613, \"y_axis\": 2.7733564376831055, \"name\": \"boo and hiss\", \"original\": \"boo and hiss\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from whatlies.language import SpacyLanguage\n",
    "from whatlies.transformers import pca \n",
    "\n",
    "lang = SpacyLanguage(\"en_core_web_sm\")\n",
    "\n",
    "contexts = [\"i am super duper happy\",\n",
    "            \"happy happy joy joy\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i am going crazy i hate it\",\n",
    "            \"boo and hiss\",]\n",
    "\n",
    "emb = lang[contexts]\n",
    "emb.transform(pca(2)).plot_interactive('pca_0', 'pca_1').properties(width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "contexts = (\"this snake is a python\",\n",
    "            \"i like to program in python\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i go to the supermarket\",\n",
    "            \"i like to code\", \n",
    "            \"i love animals\")\n",
    "\n",
    "emb = EmbeddingSet({k: Embedding(k, nlp(k).vector) for k in contexts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_str, y_str = \"python is for programming\", \"snakes are slimy creatures\"\n",
    "x_axis = Embedding(x_str, nlp(x_str).vector)\n",
    "y_axis = Embedding(y_str, nlp(y_str).vector)\n",
    "emb.plot_interactive(x_axis=x_axis, y_axis=y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings of Tokens with Context\n",
    "\n",
    "But maybe we'd like to have BERT-style models. These models work differently. Luckily ... spaCy also supports this these days. \n",
    "\n",
    "Note that you'll need to download and install this model first. You can do that by running;\n",
    "\n",
    "```\n",
    "pip install spacy-transformers\n",
    "python -m spacy download en_trf_robertabase_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_trf_robertabase_lg\")\n",
    "\n",
    "contexts = (\"this snake is a python\",\n",
    "            \"i like to program in python\",\n",
    "            \"programming is super fun!\",\n",
    "            \"i go to the supermarket\",\n",
    "            \"i like to code\", \n",
    "            \"i love animals\")\n",
    "\n",
    "t = EmbeddingSet({k: Embedding(k, nlp(k).vector) for k in contexts})\n",
    "\n",
    "x_str, y_str = \"python is for programming\", \"dogs are cool\"\n",
    "x_axis = Embedding(x_str, nlp(x_str).vector)\n",
    "y_axis = Embedding(y_str, nlp(y_str).vector)\n",
    "t.plot_interactive(x_axis=x_axis, y_axis=y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go a step further too. If we have the sentence `this snake is a python` then an algorithm like Bert will not apply seperate word embeddings for each token. Rather, the entire document will first learn it's representation before assigning it to seperate tokens. If you are interested in a Bert representation of a word given the context that it is in ... you can get them with a special syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = (\"i put my money on the [bank]\",\n",
    "            \"i put my money on the bank\",\n",
    "            \"the water flows on the river [bank]\",\n",
    "            \"the water flows on the river bank\",\n",
    "            \"i really like [to swim] in water\",\n",
    "            \"i want to be so rich that i am [drowning] in money\",\n",
    "            \"i have plenty of [cash] on me\",\n",
    "            \"money is important to my [cash] flow\", \n",
    "            \"a beach is next to the ocean\", \n",
    "            \"google gives me a wealth of information\",\n",
    "            \"that banker person is very wealthy\", \n",
    "            \"i like cats and dogs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to make use of this syntax we need a new object; the `Language` object. This is a tool for `whatlies` to grab the appropriate word embeddings on your behalf. It will handle the context but can also be seen as a lazy `EmbeddingSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from whatlies.language import SpacyLanguage\n",
    "\n",
    "lang = SpacyLanguage(\"en_trf_robertabase_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang['red'].vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these embeddings are kind of special, they depend on the context around the token of interest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(lang['Going to the [store]'].vector, \n",
    "               lang['[store] this in the drawer please.'].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also use the `EmbeddingSet` again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies.transformers import umap\n",
    "\n",
    "t = EmbeddingSet({k: lang[k] for k in contexts}).transform(umap(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = t.plot_interactive(\"i like cats and dogs\", \"i put my money on the [bank]\")\n",
    "p2 = t.plot_interactive(\"i like cats and dogs\", \"i put my money on the bank\")\n",
    "p1 | p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
